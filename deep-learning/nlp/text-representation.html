
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Word embedding &#8212; DL-learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="../../_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'deep-learning/nlp/text-representation';</script>
    <link rel="canonical" href="https://llzccz.github.io/deep-learning/deep-learning/nlp/text-representation.html" />
    <link rel="icon" href="../../_static/fav.png"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Generative adversarial networks" href="../gan.html" />
    <link rel="prev" title="Text Preprocessing" href="text-preprocessing.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.png" class="logo__image only-light" alt="DL-learning - Home"/>
    <script>document.write(`<img src="../../_static/logo.png" class="logo__image only-dark" alt="DL-learning - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">DEEP LEARNING</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../dl-overview.html">Intro to Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../nn.html">Neural Networks</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../cnn/cnn.html">Convolutional Neural Networks</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../cnn/cnn-vgg.html">Stylenet / Neural-Style</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cnn/cnn-deepdream.html">Deepdream in TensorFlow</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../rnn/rnn.html">Recurrent Neural Networks</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../rnn/lstm.html">Long-short term memory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../rnn/bi-rnn.html">Bidirectional RNN</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../time-series.html">Time series</a></li>
<li class="toctree-l1"><a class="reference internal" href="../autoencoder.html">Autoencoder</a></li>
<li class="toctree-l1"><a class="reference internal" href="../object-detection.html">Object detection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../image-classification.html">Image classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../image-segmentation.html">Image segmentation</a></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="nlp.html">Natural Language Processing</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="text-preprocessing.html">Text Preprocessing</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Word embedding</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../gan.html">Generative adversarial networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../difussion-model.html">Diffusion Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dqn.html">Deep Q-learning</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">ASSIGNMENTS</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../assignments/google-stock-price-prediction-rnn.html">Google Stock Price Prediction RNN</a></li>

<li class="toctree-l1"><a class="reference internal" href="../../assignments/bitcoin-lstm-model-with-tweet-volume-and-sentiment.html">Bitcoin LSTM Model with Tweet Volume and Sentiment</a></li>

<li class="toctree-l1"><a class="reference internal" href="../../assignments/beginner-guide-to-text-preprocessing.html">Beginnerâ€™s Guide to Text Pre-Processing</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/llzccz/deep-learning/master?urlpath=tree/deep-learning/nlp/text-representation.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onBinder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../../_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
      
      
      
      <li><a href="https://colab.research.google.com/github/llzccz/deep-learning/blob/master/deep-learning/nlp/text-representation.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/llzccz/deep-learning" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/llzccz/deep-learning/edit/main/deep-learning/nlp/text-representation.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/llzccz/deep-learning/issues/new?title=Issue%20on%20page%20%2Fdeep-learning/nlp/text-representation.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/deep-learning/nlp/text-representation.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Word embedding</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#discrete-representation">Discrete representation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#one-hot">One-Hot</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bag-of-word-bow">Bag of Word (Bow)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#term-frequency-inverse-document-frequency-tf-idf">Term frequency-inverse document frequency (TF-IDF)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#distributed-representation">Distributed representation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#word2vec">Word2Vec</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#continuous-bag-of-words-cbow">Continuous Bag of Words(CBOW)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#skip-gram">Skip-Gram</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pretrained-word-embedding">Pretrained Word-Embedding</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#glove">GloVe</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fasttext">Fasttext</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bert-bidirectional-encoder-representations-from-transformers">BERT (Bidirectional Encoder Representations from Transformers)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#considerations-for-deploying-word-embedding-models">Considerations for Deploying Word Embedding Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#advantages-and-disadvantage-of-word-embeddings">Advantages and Disadvantage of Word Embeddings</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#advantages">Advantages</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#disadvantages">Disadvantages</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#your-turn">Your turn! ðŸš€</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#acknowledgments">Acknowledgments</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Install the necessary dependencies</span>

<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="o">!{</span>sys.executable<span class="o">}</span><span class="w"> </span>-m<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>--quiet<span class="w"> </span>pandas<span class="w"> </span>scikit-learn<span class="w"> </span>numpy<span class="w"> </span>matplotlib<span class="w"> </span>jupyterlab_myst<span class="w"> </span>ipython<span class="w"> </span>gensim<span class="w"> </span>torch
</pre></div>
</div>
</div>
</details>
</div>
<section class="tex2jax_ignore mathjax_ignore" id="word-embedding">
<h1>Word embedding<a class="headerlink" href="#word-embedding" title="Link to this heading">#</a></h1>
<p>Word Embeddings are numeric representations of words in a lower-dimensional space, capturing semantic and syntactic information. Mainly including discrete representation methods and distribution representation methods</p>
<section id="discrete-representation">
<h2>Discrete representation<a class="headerlink" href="#discrete-representation" title="Link to this heading">#</a></h2>
<p>This method involves compiling a list of distinct terms and giving each one a unique integer value, or id. and after that, insert each wordâ€™s distinct id into the sentence. Every vocabulary word is handled as a feature in this instance. Thus, a large vocabulary will result in an extremely large feature size. Common discrete representation methods include:one-hot,Bag of Word (Bow) and Term frequency-inverse document frequency (TF-IDF)</p>
<section id="one-hot">
<h3>One-Hot<a class="headerlink" href="#one-hot" title="Link to this heading">#</a></h3>
<p>One-hot encoding is a simple method for representing words in natural language processing (NLP). In this encoding scheme, each word in the vocabulary is represented as a unique vector, where the dimensionality of the vector is equal to the size of the vocabulary. The vector has all elements set to 0, except for the element corresponding to the index of the word in the vocabulary, which is set to 1.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">one_hot_encode</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
	<span class="n">words</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
	<span class="n">vocabulary</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
	<span class="n">word_to_index</span> <span class="o">=</span> <span class="p">{</span><span class="n">word</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">)}</span>
	<span class="n">one_hot_encoded</span> <span class="o">=</span> <span class="p">[]</span>
	<span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
		<span class="n">one_hot_vector</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">)</span>
		<span class="n">one_hot_vector</span><span class="p">[</span><span class="n">word_to_index</span><span class="p">[</span><span class="n">word</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span>
		<span class="n">one_hot_encoded</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">one_hot_vector</span><span class="p">)</span>

	<span class="k">return</span> <span class="n">one_hot_encoded</span><span class="p">,</span> <span class="n">word_to_index</span><span class="p">,</span> <span class="n">vocabulary</span>

<span class="c1"># sample</span>
<span class="n">example_text</span> <span class="o">=</span> <span class="s2">&quot;cat in the hat dog on the mat bird in the tree&quot;</span>

<span class="n">one_hot_encoded</span><span class="p">,</span> <span class="n">word_to_index</span><span class="p">,</span> <span class="n">vocabulary</span> <span class="o">=</span> <span class="n">one_hot_encode</span><span class="p">(</span><span class="n">example_text</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Vocabulary:&quot;</span><span class="p">,</span> <span class="n">vocabulary</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Word to Index Mapping:&quot;</span><span class="p">,</span> <span class="n">word_to_index</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;One-Hot Encoded Matrix:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">encoding</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">example_text</span><span class="o">.</span><span class="n">split</span><span class="p">(),</span> <span class="n">one_hot_encoded</span><span class="p">):</span>
	<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">encoding</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Vocabulary: {&#39;mat&#39;, &#39;dog&#39;, &#39;tree&#39;, &#39;in&#39;, &#39;on&#39;, &#39;hat&#39;, &#39;the&#39;, &#39;bird&#39;, &#39;cat&#39;}
Word to Index Mapping: {&#39;mat&#39;: 0, &#39;dog&#39;: 1, &#39;tree&#39;: 2, &#39;in&#39;: 3, &#39;on&#39;: 4, &#39;hat&#39;: 5, &#39;the&#39;: 6, &#39;bird&#39;: 7, &#39;cat&#39;: 8}
One-Hot Encoded Matrix:
cat: [0, 0, 0, 0, 0, 0, 0, 0, 1]
in: [0, 0, 0, 1, 0, 0, 0, 0, 0]
the: [0, 0, 0, 0, 0, 0, 1, 0, 0]
hat: [0, 0, 0, 0, 0, 1, 0, 0, 0]
dog: [0, 1, 0, 0, 0, 0, 0, 0, 0]
on: [0, 0, 0, 0, 1, 0, 0, 0, 0]
the: [0, 0, 0, 0, 0, 0, 1, 0, 0]
mat: [1, 0, 0, 0, 0, 0, 0, 0, 0]
bird: [0, 0, 0, 0, 0, 0, 0, 1, 0]
in: [0, 0, 0, 1, 0, 0, 0, 0, 0]
the: [0, 0, 0, 0, 0, 0, 1, 0, 0]
tree: [0, 0, 1, 0, 0, 0, 0, 0, 0]
</pre></div>
</div>
</div>
</div>
</section>
<section id="bag-of-word-bow">
<h3>Bag of Word (Bow)<a class="headerlink" href="#bag-of-word-bow" title="Link to this heading">#</a></h3>
<p>Bag-of-Words (BoW) is a text representation technique that represents a document as an unordered set of words and their respective frequencies. It discards the word order and captures the frequency of each word in the document, creating a vector representation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>
<span class="n">documents</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;This is the first document.&quot;</span><span class="p">,</span>
			<span class="s2">&quot;This document is the second document.&quot;</span><span class="p">,</span>
			<span class="s2">&quot;And this is the third one.&quot;</span><span class="p">,</span>
			<span class="s2">&quot;Is this the first document?&quot;</span><span class="p">]</span>

<span class="n">vectorizer</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>
<span class="n">feature_names</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Bag-of-Words Matrix:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">toarray</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Vocabulary (Feature Names):&quot;</span><span class="p">,</span> <span class="n">feature_names</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Bag-of-Words Matrix:
[[0 1 1 1 0 0 1 0 1]
 [0 2 0 1 0 1 1 0 1]
 [1 0 0 1 1 0 1 1 1]
 [0 1 1 1 0 0 1 0 1]]
Vocabulary (Feature Names): [&#39;and&#39; &#39;document&#39; &#39;first&#39; &#39;is&#39; &#39;one&#39; &#39;second&#39; &#39;the&#39; &#39;third&#39; &#39;this&#39;]
</pre></div>
</div>
</div>
</div>
</section>
<section id="term-frequency-inverse-document-frequency-tf-idf">
<h3>Term frequency-inverse document frequency (TF-IDF)<a class="headerlink" href="#term-frequency-inverse-document-frequency-tf-idf" title="Link to this heading">#</a></h3>
<p>Term Frequency-Inverse Document Frequency, commonly known as TF-IDF, is a numerical statistic that reflects the importance of a word in a document relative to a collection of documents (corpus). It is widely used in natural language processing and information retrieval to evaluate the significance of a term within a specific document in a larger corpus. TF-IDF consists of two components:</p>
<p><strong>Â·Term Frequency (TF)</strong>: Term Frequency measures how often a term (word) appears in a document. It is calculated using the formula:</p>
<p><span class="math notranslate nohighlight">\(TF(t,d)=\frac{Total\ number\ of\ times\ term\ t\ appears\ in\ document\ d}{Total\ number\ of\ terms\ in\ document\ d}\)</span></p>
<p><strong>Â·Inverse Document Frequency (IDF)</strong>: Inverse Document Frequency measures the importance of a term across a collection of documents. It is calculated using the formula:</p>
<p><span class="math notranslate nohighlight">\(IDF(t,D)=\log{(\frac{Total\ documents}{Number\ of\ documents\ containing\ term\ t})}\)</span></p>
<p>The TF-IDF score for a term t in a document d is then given by multiplying the TF and IDF values:</p>
<p>TF-IDF(t,d,D)=TF(t,d)Ã—IDF(t,D)</p>
<p>The higher the TF-IDF score for a term in a document, the more important that term is to that document within the context of the entire corpus. This weighting scheme helps in identifying and extracting relevant information from a large collection of documents, and it is commonly used in text mining, information retrieval, and document clustering.</p>
<p>Letâ€™s Implement Term Frequency-Inverse Document Frequency (TF-IDF) using python with the scikit-learn library. It begins by defining a set of sample documents. The TfidfVectorizer is employed to transform these documents into a TF-IDF matrix. The code then extracts and prints the TF-IDF values for each word in each document. This statistical measure helps assess the importance of words in a document relative to their frequency across a collection of documents, aiding in information retrieval and text analysis tasks.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span>

<span class="c1"># Sample</span>
<span class="n">documents</span> <span class="o">=</span> <span class="p">[</span>
	<span class="s2">&quot;The quick brown fox jumps over the lazy dog.&quot;</span><span class="p">,</span>
	<span class="s2">&quot;A journey of a thousand miles begins with a single step.&quot;</span><span class="p">,</span>
<span class="p">]</span>

<span class="n">vectorizer</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">()</span> <span class="c1"># Create the TF-IDF vectorizer</span>
<span class="n">tfidf_matrix</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>
<span class="n">feature_names</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">()</span>
<span class="n">tfidf_values</span> <span class="o">=</span> <span class="p">{}</span>

<span class="k">for</span> <span class="n">doc_index</span><span class="p">,</span> <span class="n">doc</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">documents</span><span class="p">):</span>
	<span class="n">feature_index</span> <span class="o">=</span> <span class="n">tfidf_matrix</span><span class="p">[</span><span class="n">doc_index</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">nonzero</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span>
	<span class="n">tfidf_doc_values</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="n">feature_index</span><span class="p">,</span> <span class="p">[</span><span class="n">tfidf_matrix</span><span class="p">[</span><span class="n">doc_index</span><span class="p">,</span> <span class="n">x</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">feature_index</span><span class="p">])</span>
	<span class="n">tfidf_values</span><span class="p">[</span><span class="n">doc_index</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="n">feature_names</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span> <span class="n">value</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">tfidf_doc_values</span><span class="p">}</span>
<span class="c1">#let&#39;s print</span>
<span class="k">for</span> <span class="n">doc_index</span><span class="p">,</span> <span class="n">values</span> <span class="ow">in</span> <span class="n">tfidf_values</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
	<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Document </span><span class="si">{</span><span class="n">doc_index</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">:&quot;</span><span class="p">)</span>
	<span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">tfidf_value</span> <span class="ow">in</span> <span class="n">values</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
		<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">tfidf_value</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
	<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Document 1:
dog: 0.30151134457776363
lazy: 0.30151134457776363
over: 0.30151134457776363
jumps: 0.30151134457776363
fox: 0.30151134457776363
brown: 0.30151134457776363
quick: 0.30151134457776363
the: 0.6030226891555273


Document 2:
step: 0.3535533905932738
single: 0.3535533905932738
with: 0.3535533905932738
begins: 0.3535533905932738
miles: 0.3535533905932738
thousand: 0.3535533905932738
of: 0.3535533905932738
journey: 0.3535533905932738
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="distributed-representation">
<h2>Distributed representation<a class="headerlink" href="#distributed-representation" title="Link to this heading">#</a></h2>
<section id="word2vec">
<h3>Word2Vec<a class="headerlink" href="#word2vec" title="Link to this heading">#</a></h3>
<p>Word2Vec is a neural approach for generating word embeddings. It belongs to the family of neural word embedding techniques and specifically falls under the category of distributed representation models. It is a popular technique in natural language processing (NLP) that is used to represent words as continuous vector spaces. Developed by a team at Google, Word2Vec aims to capture the semantic relationships between words by mapping them to high-dimensional vectors. The underlying idea is that words with similar meanings should have similar vector representations. In Word2Vec every word is assigned a vector. We start with either a random vector or one-hot vector.</p>
<p>There are two neural embedding methods for Word2Vec, Continuous Bag of Words (CBOW) and Skip-gram.</p>
<section id="continuous-bag-of-words-cbow">
<h4>Continuous Bag of Words(CBOW)<a class="headerlink" href="#continuous-bag-of-words-cbow" title="Link to this heading">#</a></h4>
<p>Continuous Bag of Words (CBOW) is a type of neural network architecture used in the Word2Vec model. The primary objective of CBOW is to predict a target word based on its context, which consists of the surrounding words in a given window. Given a sequence of words in a context window, the model is trained to predict the target word at the center of the window.</p>
<p>CBOW is a feedforward neural network with a single hidden layer. The input layer represents the context words, and the output layer represents the target word. The hidden layer contains the learned continuous vector representations (word embeddings) of the input words.</p>
<p>The architecture is useful for learning distributed representations of words in a continuous vector space.</p>
<figure class="align-default" id="continuous-bag-of-words">
<a class="reference internal image-reference" href="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/deep-learning/NLP/cbow.png"><img alt="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/deep-learning/NLP/cbow.png" src="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/deep-learning/NLP/cbow.png" style="width: 90%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 44 </span><span class="caption-text">Continuous Bag of Words</span><a class="headerlink" href="#continuous-bag-of-words" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The hidden layer contains the continuous vector representations (word embeddings) of the input words.</p>
<p>The weights between the input layer and the hidden layer are learned during training.
The dimensionality of the hidden layer represents the size of the word embeddings (the continuous vector space).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>

<span class="c1"># Define CBOW model</span>
<span class="k">class</span> <span class="nc">CBOWModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
	<span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">):</span>
		<span class="nb">super</span><span class="p">(</span><span class="n">CBOWModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">)</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_size</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>

	<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">):</span>
		<span class="n">context_embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span><span class="p">(</span><span class="n">context</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
		<span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">context_embeds</span><span class="p">)</span>
		<span class="k">return</span> <span class="n">output</span>

<span class="c1"># Sample data</span>
<span class="n">context_size</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">raw_text</span> <span class="o">=</span> <span class="s2">&quot;word embeddings are awesome&quot;</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="n">raw_text</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
<span class="n">vocab</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
<span class="n">word_to_index</span> <span class="o">=</span> <span class="p">{</span><span class="n">word</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vocab</span><span class="p">)}</span>
<span class="n">data</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span><span class="p">):</span>
	<span class="n">context</span> <span class="o">=</span> <span class="p">[</span><span class="n">word_to_index</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">2</span><span class="p">:</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">tokens</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="mi">3</span><span class="p">]]</span>
	<span class="n">target</span> <span class="o">=</span> <span class="n">word_to_index</span><span class="p">[</span><span class="n">tokens</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span>
	<span class="n">data</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">context</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">target</span><span class="p">)))</span>

<span class="c1"># Hyperparameters</span>
<span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>
<span class="n">embed_size</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">100</span>

<span class="c1"># Initialize CBOW model</span>
<span class="n">cbow_model</span> <span class="o">=</span> <span class="n">CBOWModel</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">cbow_model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>

<span class="c1"># Training loop</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
	<span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
	<span class="k">for</span> <span class="n">context</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
		<span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
		<span class="n">output</span> <span class="o">=</span> <span class="n">cbow_model</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>
		<span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">target</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
		<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
		<span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
		<span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
	<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">, Loss: </span><span class="si">{</span><span class="n">total_loss</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Example usage: Get embedding for a specific word</span>
<span class="n">word_to_lookup</span> <span class="o">=</span> <span class="s2">&quot;embeddings&quot;</span>
<span class="n">word_index</span> <span class="o">=</span> <span class="n">word_to_index</span><span class="p">[</span><span class="n">word_to_lookup</span><span class="p">]</span>
<span class="n">embedding</span> <span class="o">=</span> <span class="n">cbow_model</span><span class="o">.</span><span class="n">embeddings</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">word_index</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Embedding for &#39;</span><span class="si">{</span><span class="n">word_to_lookup</span><span class="si">}</span><span class="s2">&#39;: </span><span class="si">{</span><span class="n">embedding</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1, Loss: 0
Epoch 2, Loss: 0
Epoch 3, Loss: 0
Epoch 4, Loss: 0
Epoch 5, Loss: 0
Epoch 6, Loss: 0
Epoch 7, Loss: 0
Epoch 8, Loss: 0
Epoch 9, Loss: 0
Epoch 10, Loss: 0
Epoch 11, Loss: 0
Epoch 12, Loss: 0
Epoch 13, Loss: 0
Epoch 14, Loss: 0
Epoch 15, Loss: 0
Epoch 16, Loss: 0
Epoch 17, Loss: 0
Epoch 18, Loss: 0
Epoch 19, Loss: 0
Epoch 20, Loss: 0
Epoch 21, Loss: 0
Epoch 22, Loss: 0
Epoch 23, Loss: 0
Epoch 24, Loss: 0
Epoch 25, Loss: 0
Epoch 26, Loss: 0
Epoch 27, Loss: 0
Epoch 28, Loss: 0
Epoch 29, Loss: 0
Epoch 30, Loss: 0
Epoch 31, Loss: 0
Epoch 32, Loss: 0
Epoch 33, Loss: 0
Epoch 34, Loss: 0
Epoch 35, Loss: 0
Epoch 36, Loss: 0
Epoch 37, Loss: 0
Epoch 38, Loss: 0
Epoch 39, Loss: 0
Epoch 40, Loss: 0
Epoch 41, Loss: 0
Epoch 42, Loss: 0
Epoch 43, Loss: 0
Epoch 44, Loss: 0
Epoch 45, Loss: 0
Epoch 46, Loss: 0
Epoch 47, Loss: 0
Epoch 48, Loss: 0
Epoch 49, Loss: 0
Epoch 50, Loss: 0
Epoch 51, Loss: 0
Epoch 52, Loss: 0
Epoch 53, Loss: 0
Epoch 54, Loss: 0
Epoch 55, Loss: 0
Epoch 56, Loss: 0
Epoch 57, Loss: 0
Epoch 58, Loss: 0
Epoch 59, Loss: 0
Epoch 60, Loss: 0
Epoch 61, Loss: 0
Epoch 62, Loss: 0
Epoch 63, Loss: 0
Epoch 64, Loss: 0
Epoch 65, Loss: 0
Epoch 66, Loss: 0
Epoch 67, Loss: 0
Epoch 68, Loss: 0
Epoch 69, Loss: 0
Epoch 70, Loss: 0
Epoch 71, Loss: 0
Epoch 72, Loss: 0
Epoch 73, Loss: 0
Epoch 74, Loss: 0
Epoch 75, Loss: 0
Epoch 76, Loss: 0
Epoch 77, Loss: 0
Epoch 78, Loss: 0
Epoch 79, Loss: 0
Epoch 80, Loss: 0
Epoch 81, Loss: 0
Epoch 82, Loss: 0
Epoch 83, Loss: 0
Epoch 84, Loss: 0
Epoch 85, Loss: 0
Epoch 86, Loss: 0
Epoch 87, Loss: 0
Epoch 88, Loss: 0
Epoch 89, Loss: 0
Epoch 90, Loss: 0
Epoch 91, Loss: 0
Epoch 92, Loss: 0
Epoch 93, Loss: 0
Epoch 94, Loss: 0
Epoch 95, Loss: 0
Epoch 96, Loss: 0
Epoch 97, Loss: 0
Epoch 98, Loss: 0
Epoch 99, Loss: 0
Epoch 100, Loss: 0
Embedding for &#39;embeddings&#39;: [[-1.0344244   0.07897425  0.03608504 -0.556515    0.14206451 -1.833232
   0.7083351   0.66258     0.57552516  1.7009653 ]]
</pre></div>
</div>
</div>
</div>
</section>
<section id="skip-gram">
<h4>Skip-Gram<a class="headerlink" href="#skip-gram" title="Link to this heading">#</a></h4>
<p>The Skip-Gram model learns distributed representations of words in a continuous vector space. The main objective of Skip-Gram is to predict context words (words surrounding a target word) given a target word. This is the opposite of the Continuous Bag of Words (CBOW) model, where the objective is to predict the target word based on its context. It is shown that this method produces more meaningful embeddings.</p>
<figure class="align-default" id="id1">
<a class="reference internal image-reference" href="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/deep-learning/NLP/skipgram.png"><img alt="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/deep-learning/NLP/skipgram.png" src="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/deep-learning/NLP/skipgram.png" style="width: 90%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 45 </span><span class="caption-text">Skip Gram</span><a class="headerlink" href="#id1" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>After applying the above neural embedding methods we get trained vectors of each word after many iterations through the corpus. These trained vectors preserve syntactical or semantic information and are converted to lower dimensions. The vectors with similar meaning or semantic information are placed close to each other in space.</p>
<p>Letâ€™s understand with a basic example. The python code contains, parameter that controls the dimensionality of the word vectors, and you can adjust other parameters such as based on your specific needs.vector_sizewindow</p>
<blockquote>
<div><p>Note: Word2Vec models can perform better with larger datasets.
If you have a large corpus, you might achieve more meaningful word embeddings.</p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>pip<span class="w"> </span>install<span class="w"> </span>gensim
<span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">Word2Vec</span>
<span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">word_tokenize</span>
<span class="kn">import</span> <span class="nn">nltk</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;punkt&#39;</span><span class="p">)</span> <span class="c1"># Download the tokenizer models if not already downloaded</span>

<span class="n">sample</span> <span class="o">=</span> <span class="s2">&quot;Word embeddings are dense vector representations of words.&quot;</span>
<span class="n">tokenized_corpus</span> <span class="o">=</span> <span class="n">word_tokenize</span><span class="p">(</span><span class="n">sample</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span> <span class="c1"># Lowercasing for consistency</span>

<span class="n">skipgram_model</span> <span class="o">=</span> <span class="n">Word2Vec</span><span class="p">(</span><span class="n">sentences</span><span class="o">=</span><span class="p">[</span><span class="n">tokenized_corpus</span><span class="p">],</span>
						<span class="n">vector_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="c1"># Dimensionality of the word vectors</span>
						<span class="n">window</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>		 <span class="c1"># Maximum distance between the current and predicted word within a sentence</span>
						<span class="n">sg</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>			 <span class="c1"># Skip-Gram model (1 for Skip-Gram, 0 for CBOW)</span>
						<span class="n">min_count</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>	 <span class="c1"># Ignores all words with a total frequency lower than this</span>
						<span class="n">workers</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>	 <span class="c1"># Number of CPU cores to use for training the model</span>

<span class="c1"># Training</span>
<span class="n">skipgram_model</span><span class="o">.</span><span class="n">train</span><span class="p">([</span><span class="n">tokenized_corpus</span><span class="p">],</span> <span class="n">total_examples</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">skipgram_model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&quot;skipgram_model.model&quot;</span><span class="p">)</span>
<span class="n">loaded_model</span> <span class="o">=</span> <span class="n">Word2Vec</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;skipgram_model.model&quot;</span><span class="p">)</span>
<span class="n">vector_representation</span> <span class="o">=</span> <span class="n">loaded_model</span><span class="o">.</span><span class="n">wv</span><span class="p">[</span><span class="s1">&#39;word&#39;</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Vector representation of &#39;word&#39;:&quot;</span><span class="p">,</span> <span class="n">vector_representation</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Requirement already satisfied: gensim in d:\anaconda\lib\site-packages (4.3.0)
Requirement already satisfied: FuzzyTM&gt;=0.4.0 in d:\anaconda\lib\site-packages (from gensim) (2.0.5)
Requirement already satisfied: numpy&gt;=1.18.5 in d:\anaconda\lib\site-packages (from gensim) (1.23.5)
Requirement already satisfied: scipy&gt;=1.7.0 in d:\anaconda\lib\site-packages (from gensim) (1.10.0)
Requirement already satisfied: smart-open&gt;=1.8.1 in d:\anaconda\lib\site-packages (from gensim) (5.2.1)
Requirement already satisfied: pyfume in d:\anaconda\lib\site-packages (from FuzzyTM&gt;=0.4.0-&gt;gensim) (0.2.25)
Requirement already satisfied: pandas in d:\anaconda\lib\site-packages (from FuzzyTM&gt;=0.4.0-&gt;gensim) (1.5.3)
Requirement already satisfied: python-dateutil&gt;=2.8.1 in d:\anaconda\lib\site-packages (from pandas-&gt;FuzzyTM&gt;=0.4.0-&gt;gensim) (2.8.2)
Requirement already satisfied: pytz&gt;=2020.1 in d:\anaconda\lib\site-packages (from pandas-&gt;FuzzyTM&gt;=0.4.0-&gt;gensim) (2022.7)
Requirement already satisfied: fst-pso in d:\anaconda\lib\site-packages (from pyfume-&gt;FuzzyTM&gt;=0.4.0-&gt;gensim) (1.8.1)
Requirement already satisfied: simpful in d:\anaconda\lib\site-packages (from pyfume-&gt;FuzzyTM&gt;=0.4.0-&gt;gensim) (2.11.0)
Requirement already satisfied: six&gt;=1.5 in d:\anaconda\lib\site-packages (from python-dateutil&gt;=2.8.1-&gt;pandas-&gt;FuzzyTM&gt;=0.4.0-&gt;gensim) (1.16.0)
Requirement already satisfied: miniful in d:\anaconda\lib\site-packages (from fst-pso-&gt;pyfume-&gt;FuzzyTM&gt;=0.4.0-&gt;gensim) (0.0.6)
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[nltk_data] Downloading package punkt to
[nltk_data]     C:\Users\zhongmeiqi\AppData\Roaming\nltk_data...
[nltk_data]   Package punkt is already up-to-date!
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Vector representation of &#39;word&#39;: [-9.5800208e-03  8.9437785e-03  4.1664648e-03  9.2367809e-03
  6.6457358e-03  2.9233587e-03  9.8055992e-03 -4.4231843e-03
 -6.8048164e-03  4.2256550e-03  3.7299085e-03 -5.6668529e-03
  9.7035142e-03 -3.5551414e-03  9.5499391e-03  8.3657773e-04
 -6.3355025e-03 -1.9741615e-03 -7.3781307e-03 -2.9811086e-03
  1.0425397e-03  9.4814906e-03  9.3598543e-03 -6.5986011e-03
  3.4773252e-03  2.2767992e-03 -2.4910474e-03 -9.2290826e-03
  1.0267317e-03 -8.1645092e-03  6.3240929e-03 -5.8001447e-03
  5.5353874e-03  9.8330071e-03 -1.5987856e-04  4.5296676e-03
 -1.8086446e-03  7.3613892e-03  3.9419360e-03 -9.0095028e-03
 -2.3953868e-03  3.6261671e-03 -1.0080514e-04 -1.2024897e-03
 -1.0558038e-03 -1.6681013e-03  6.0541567e-04  4.1633579e-03
 -4.2531900e-03 -3.8336846e-03 -5.0755290e-05  2.6549282e-04
 -1.7014991e-04 -4.7843382e-03  4.3120929e-03 -2.1710952e-03
  2.1056964e-03  6.6702347e-04  5.9686624e-03 -6.8418151e-03
 -6.8183104e-03 -4.4762432e-03  9.4359247e-03 -1.5930856e-03
 -9.4291316e-03 -5.4270827e-04 -4.4478951e-03  5.9980620e-03
 -9.5831212e-03  2.8602476e-03 -9.2544509e-03  1.2484600e-03
  6.0004774e-03  7.4001122e-03 -7.6209377e-03 -6.0561695e-03
 -6.8399287e-03 -7.9184016e-03 -9.4984965e-03 -2.1255787e-03
 -8.3757477e-04 -7.2564054e-03  6.7876028e-03  1.1183097e-03
  5.8291717e-03  1.4714618e-03  7.9081533e-04 -7.3718326e-03
 -2.1769912e-03  4.3199472e-03 -5.0856168e-03  1.1304744e-03
  2.8835384e-03 -1.5386029e-03  9.9318363e-03  8.3507905e-03
  2.4184163e-03  7.1170190e-03  5.8888551e-03 -5.5787875e-03]
</pre></div>
</div>
</div>
</div>
<p>In practice, the choice between CBOW and Skip-gram often depends on the specific characteristics of the data and the task at hand. CBOW might be preferred when training resources are limited, and capturing syntactic information is important. Skip-gram, on the other hand, might be chosen when semantic relationships and the representation of rare words are crucial.</p>
</section>
</section>
</section>
<section id="pretrained-word-embedding">
<h2>Pretrained Word-Embedding<a class="headerlink" href="#pretrained-word-embedding" title="Link to this heading">#</a></h2>
<p>Pre-trained word embeddings are representations of words that are learned from large corpora and are made available for reuse in various natural language processing (NLP) tasks. These embeddings capture semantic relationships between words, allowing the model to understand similarities and relationships between different words in a meaningful way.</p>
<section id="glove">
<h3>GloVe<a class="headerlink" href="#glove" title="Link to this heading">#</a></h3>
<p>GloVe is trained on global word co-occurrence statistics. It leverages the global context to create word embeddings that reflect the overall meaning of words based on their co-occurrence probabilities. this method, we take the corpus and iterate through it and get the co-occurrence of each word with other words in the corpus. We get a co-occurrence matrix through this. The words which occur next to each other get a value of 1, if they are one word apart then 1/2, if two words apart then 1/3 and so on.</p>
<p>Let us take an example to understand how the matrix is created. We have a small corpus:</p>
<blockquote>
<div><p>Corpus:</p>
<p>It is a nice evening.</p>
<p>Good Evening!</p>
<p>Is it a nice evening?</p>
</div></blockquote>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>it</p></th>
<th class="head"><p>is</p></th>
<th class="head"><p>a</p></th>
<th class="head"><p>nice</p></th>
<th class="head"><p>evening</p></th>
<th class="head"><p>good</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>it</p></td>
<td><p>0</p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>is</p></td>
<td><p>1+1</p></td>
<td><p>0</p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p>a</p></td>
<td><p>1/2+1</p></td>
<td><p>1+1/2</p></td>
<td><p>0</p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>nice</p></td>
<td><p>1/3+1/2</p></td>
<td><p>1/2+1/3</p></td>
<td><p>1+1</p></td>
<td><p>0</p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p>evening</p></td>
<td><p>1/4+1/3</p></td>
<td><p>1/3+1/4</p></td>
<td><p>1/2+1/2</p></td>
<td><p>1+1</p></td>
<td><p>0</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>good</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
</tr>
</tbody>
</table>
<p>The upper half of the matrix will be a reflection of the lower half. We can consider a window frame as well to calculate the co-occurrences by shifting the frame till the end of the corpus. This helps gather information about the context in which the word is used.</p>
<p>Initially, the vectors for each word is assigned randomly. Then we take two pairs of vectors and see how close they are to each other in space. If they occur together more often or have a higher value in the co-occurrence matrix and are far apart in space then they are brought close to each other. If they are close to each other but are rarely or not frequently used together then they are moved further apart in space.</p>
<p>After many iterations of the above process, weâ€™ll get a vector space representation that approximates the information from the co-occurrence matrix. The performance of GloVe is better than Word2Vec in terms of both semantic and syntactic capturing.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>
<span class="kn">from</span> <span class="nn">gensim.downloader</span> <span class="kn">import</span> <span class="n">load</span>

<span class="n">glove_model</span> <span class="o">=</span> <span class="n">load</span><span class="p">(</span><span class="s1">&#39;glove-wiki-gigaword-50&#39;</span><span class="p">)</span>
<span class="n">word_pairs</span> <span class="o">=</span> <span class="p">[(</span><span class="s1">&#39;learn&#39;</span><span class="p">,</span> <span class="s1">&#39;learning&#39;</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;india&#39;</span><span class="p">,</span> <span class="s1">&#39;indian&#39;</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;fame&#39;</span><span class="p">,</span> <span class="s1">&#39;famous&#39;</span><span class="p">)]</span>

<span class="c1"># Compute similarity for each pair of words</span>
<span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">word_pairs</span><span class="p">:</span>
	<span class="n">similarity</span> <span class="o">=</span> <span class="n">glove_model</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
	<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Similarity between &#39;</span><span class="si">{</span><span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&#39; and &#39;</span><span class="si">{</span><span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">&#39; using GloVe: </span><span class="si">{</span><span class="n">similarity</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Output:</p>
<blockquote>
<div><p>Similarity between â€˜learnâ€™ and â€˜learningâ€™ using GloVe: 0.802</p>
<p>Similarity between â€˜indiaâ€™ and â€˜indianâ€™ using GloVe: 0.865</p>
<p>Similarity between â€˜fameâ€™ and â€˜famousâ€™ using GloVe: 0.589</p>
</div></blockquote>
</section>
<section id="fasttext">
<h3>Fasttext<a class="headerlink" href="#fasttext" title="Link to this heading">#</a></h3>
<p>Developed by Facebook, FastText extends Word2Vec by representing words as bags of character n-grams. This approach is particularly useful for handling out-of-vocabulary words and capturing morphological variations.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">gensim.downloader</span> <span class="k">as</span> <span class="nn">api</span>
<span class="n">fasttext_model</span> <span class="o">=</span> <span class="n">api</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;fasttext-wiki-news-subwords-300&quot;</span><span class="p">)</span> <span class="c1">## Load the pre-trained fastText model</span>
<span class="c1"># Define word pairs to compute similarity for</span>
<span class="n">word_pairs</span> <span class="o">=</span> <span class="p">[(</span><span class="s1">&#39;learn&#39;</span><span class="p">,</span> <span class="s1">&#39;learning&#39;</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;india&#39;</span><span class="p">,</span> <span class="s1">&#39;indian&#39;</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;fame&#39;</span><span class="p">,</span> <span class="s1">&#39;famous&#39;</span><span class="p">)]</span>

<span class="c1"># Compute similarity for each pair of words</span>
<span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">word_pairs</span><span class="p">:</span>
	<span class="n">similarity</span> <span class="o">=</span> <span class="n">fasttext_model</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
	<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Similarity between &#39;</span><span class="si">{</span><span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&#39; and &#39;</span><span class="si">{</span><span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">&#39; using FastText: </span><span class="si">{</span><span class="n">similarity</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Output:</p>
<blockquote>
<div><p>Similarity between â€˜learnâ€™ and â€˜learningâ€™ using Word2Vec: 0.642</p>
<p>Similarity between â€˜indiaâ€™ and â€˜indianâ€™ using Word2Vec: 0.708</p>
<p>Similarity between â€˜fameâ€™ and â€˜famousâ€™ using Word2Vec: 0.519</p>
</div></blockquote>
</section>
<section id="bert-bidirectional-encoder-representations-from-transformers">
<h3>BERT (Bidirectional Encoder Representations from Transformers)<a class="headerlink" href="#bert-bidirectional-encoder-representations-from-transformers" title="Link to this heading">#</a></h3>
<p>BERT is a transformer-based model that learns contextualized embeddings for words. It considers the entire context of a word by considering both left and right contexts, resulting in embeddings that capture rich contextual information.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">BertTokenizer</span><span class="p">,</span> <span class="n">BertModel</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="c1"># Load pre-trained BERT model and tokenizer</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s1">&#39;bert-base-uncased&#39;</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>

<span class="n">word_pairs</span> <span class="o">=</span> <span class="p">[(</span><span class="s1">&#39;learn&#39;</span><span class="p">,</span> <span class="s1">&#39;learning&#39;</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;india&#39;</span><span class="p">,</span> <span class="s1">&#39;indian&#39;</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;fame&#39;</span><span class="p">,</span> <span class="s1">&#39;famous&#39;</span><span class="p">)]</span>

<span class="c1"># Compute similarity for each pair of words</span>
<span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">word_pairs</span><span class="p">:</span>
	<span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">pair</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">&#39;pt&#39;</span><span class="p">)</span>
	<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
		<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">tokens</span><span class="p">)</span>
	
	<span class="c1"># Extract embeddings for the [CLS] token</span>
	<span class="n">cls_embedding</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">last_hidden_state</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:]</span>

	<span class="n">similarity</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">cosine_similarity</span><span class="p">(</span><span class="n">cls_embedding</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">cls_embedding</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
	
	<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Similarity between &#39;</span><span class="si">{</span><span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&#39; and &#39;</span><span class="si">{</span><span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">&#39; using BERT: </span><span class="si">{</span><span class="n">similarity</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Output:</p>
<blockquote>
<div><p>Similarity between â€˜learnâ€™ and â€˜learningâ€™ using BERT: 0.930</p>
<p>Similarity between â€˜indiaâ€™ and â€˜indianâ€™ using BERT: 0.957</p>
<p>Similarity between â€˜fameâ€™ and â€˜famousâ€™ using BERT: 0.956</p>
</div></blockquote>
</section>
</section>
<section id="considerations-for-deploying-word-embedding-models">
<h2>Considerations for Deploying Word Embedding Models<a class="headerlink" href="#considerations-for-deploying-word-embedding-models" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>You need to use the exact same pipeline during deploying your model as were used to create the training data for the word embedding. If you use a different tokenizer or different method of handling white space, punctuation etc. you might end up with incompatible inputs.</p></li>
</ul>
<ul class="simple">
<li><p>Words in your input that doesnâ€™t have a pre-trained vector. Such words are known as Out of Vocabulary Word(oov). What you can do is replace those words with â€œUNKâ€ which means unknown and then handle them separately.</p></li>
</ul>
<ul class="simple">
<li><p>Dimension mis-match: Vectors can be of many lengths. If you train a model with vectors of length say 400 and then try to apply vectors of length 1000 at inference time, you will run into errors. So make sure to use the same dimensions throughout.</p></li>
</ul>
</section>
<section id="advantages-and-disadvantage-of-word-embeddings">
<h2>Advantages and Disadvantage of Word Embeddings<a class="headerlink" href="#advantages-and-disadvantage-of-word-embeddings" title="Link to this heading">#</a></h2>
<section id="advantages">
<h3>Advantages<a class="headerlink" href="#advantages" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>It is much faster to train than hand build models like WordNet (which uses graph embeddings).</p></li>
<li><p>Almost all modern NLP applications start with an embedding layer.</p></li>
<li><p>It Stores an approximation of meaning.</p></li>
</ul>
</section>
<section id="disadvantages">
<h3>Disadvantages<a class="headerlink" href="#disadvantages" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>It can be memory intensive.</p></li>
<li><p>It is corpus dependent. Any underlying bias will have an effect on your model.</p></li>
<li><p>It cannot distinguish between homophones. Eg: brake/break, cell/sell, weather/whether etc.</p></li>
</ul>
</section>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading">#</a></h2>
<p>In conclusion, word embedding techniques such as TF-IDF, Word2Vec, and GloVe play a crucial role in natural language processing by representing words in a lower-dimensional space, capturing semantic and syntactic information.</p>
</section>
<section id="your-turn">
<h2>Your turn! ðŸš€<a class="headerlink" href="#your-turn" title="Link to this heading">#</a></h2>
<p>Assignment - <a class="reference internal" href="#../../assignments/deep-learning/nlp/news-topic-classification-tasks.ipynb"><span class="xref myst">News topic classification tasks</span></a></p>
</section>
<section id="acknowledgments">
<h2>Acknowledgments<a class="headerlink" href="#acknowledgments" title="Link to this heading">#</a></h2>
<p>Thanks to <a class="reference external" href="https://auth.geeksforgeeks.org/user/shristikotaiah/articles?utm_source=geeksforgeeks&amp;amp;utm_medium=article_author&amp;amp;utm_campaign=auth_user">GeeksforGeeks</a> for creating the open-source project <a class="reference external" href="https://www.geeksforgeeks.org/word-embeddings-in-nlp">Word Embeddings in NLP</a>.It inspire the majority of the content in this chapter.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./deep-learning/nlp"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="text-preprocessing.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Text Preprocessing</p>
      </div>
    </a>
    <a class="right-next"
       href="../gan.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Generative adversarial networks</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#discrete-representation">Discrete representation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#one-hot">One-Hot</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bag-of-word-bow">Bag of Word (Bow)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#term-frequency-inverse-document-frequency-tf-idf">Term frequency-inverse document frequency (TF-IDF)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#distributed-representation">Distributed representation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#word2vec">Word2Vec</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#continuous-bag-of-words-cbow">Continuous Bag of Words(CBOW)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#skip-gram">Skip-Gram</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pretrained-word-embedding">Pretrained Word-Embedding</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#glove">GloVe</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fasttext">Fasttext</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bert-bidirectional-encoder-representations-from-transformers">BERT (Bidirectional Encoder Representations from Transformers)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#considerations-for-deploying-word-embedding-models">Considerations for Deploying Word Embedding Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#advantages-and-disadvantage-of-word-embeddings">Advantages and Disadvantage of Word Embeddings</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#advantages">Advantages</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#disadvantages">Disadvantages</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#your-turn">Your turn! ðŸš€</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#acknowledgments">Acknowledgments</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Xinyu Li
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      Â© Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>